## 'robots.txt' 

'robots.txt' 파일은 검색엔진이 사이트의 일부에 액세스하여 크롤링할 수 있는지를 알려준다.

robots.txt는 검색로봇에게 사이트 및 웹페이지를 수집할 수 있도록 허용하거나 제한하는 국제 권고안이다.

해당 파일 내에 크롤링 허용 범위 등을 적어 놓으면 된다. 일부 페이지만 크롤링 가능하게 설정할 수도 있고, 특정 검색 로봇만 크롤링 가능하도록 설정할 수도 있다.

```
//ex. 모든 검색엔진의 로봇에 대하여 수집 허용

User-agent: *
Allow: /

```

## 주의 사항
- 이 파일의 이름은 'robots.txt'이어야 하며 반드시 루트 디렉토리에 위치해야 한다. (Next.js의 경우 public 폴더 안에 위치하면 된다.)

- 로봇 배제 표준을 따르는 일반 텍스트 파일로 작성해야 한다. 

## 피해야 할 사항 (구글 Search console 고객센터 출처)

- 내부 검색결과 페이지가 Google에 크롤링되지 않도록 하시기 바랍니다. 
사용자는 검색엔진의 검색결과를 클릭했을 때 내 사이트에 표시된 다른 검색결과 페이지로 이어지는 것을 좋아하지 않습니다.

- 프록시 서비스의 결과로 생성된 URL이 크롤링되도록 허용하는 경우
민감한 정보에는 보다 안전한 방법을 사용하세요.

- robots.txt는 민감하거나 기밀인 자료를 차단하기에 적절하거나 효과적인 방법이 아닙니다. 
robots.txt는 잘 작동하는 크롤러에 크롤링 대상이 아닌 페이지를 알려주지만, 서버가 페이지를 요청하는 브라우저에 페이지를 게시하는 것을 막지는 않습니다. 

인터넷 어딘가에 내가 차단한 URL의 링크가 있는 경우 검색엔진에서 이 URL(제목이나 스니펫이 아닌 URL만 표시됨)을 계속 참조할 수 있기 때문입니다. 

또한 로봇 배제 표준을 준수 또는 인정하지 않는 불량 검색엔진에서는 robots.txt의 지시를 따르지 않을 수도 있습니다. 

마지막으로 호기심이 많은 사용자가 robots.txt 파일에 있는 디렉토리 또는 하위 디렉토리를 확인하고 내가 표시하지 않으려는 콘텐츠의 URL을 추측할 수 있습니다.

- 페이지가 Google에 표시되는 것은 원하지 않지만 링크가 있는 사용자가 페이지에 액세스하는 것은 상관하지 않는 경우 noindex 태그를 사용하세요. 

실제로 보안을 강화하려면 사용자 비밀번호를 요구하거나 사이트에서 페이지를 완전히 제외시키는 등의 적절한 인증 방법을 사용해야 합니다.
